{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple Pytorch Tutorial for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this tutorial, we will explore how to build a custom Pytorch CNN with a sklearn-like API.  \n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will create a dataset comprising images of the digits 0 and 1. These images will serve as training data for a basic neural network tasked with classifying these digits.\n",
    "<br>\n",
    "\n",
    "Specifically, X will represent a numpy array with dimensions (Number_of_images x 28 x 28 x 3), while y will be a numpy array sized (Number_of_images x 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Function to generate an image of digit 0 with random size and position\n",
    "def generate_zero_image(image_size):\n",
    "    image = np.zeros((image_size, image_size, 3), dtype=np.float32)\n",
    "    size = np.random.randint(image_size // 2, image_size)  # Random size for digit 0\n",
    "    x_pos = np.random.randint(0, image_size - size)\n",
    "    y_pos = np.random.randint(0, image_size - size)\n",
    "    color = np.random.randint(0, 256, size=3)  # Random color\n",
    "    cv2.circle(image, (x_pos + size // 2, y_pos + size // 2), size // 4, color.tolist(), thickness=-1)\n",
    "    return image\n",
    "\n",
    "# Function to generate an image of digit 1 with random size and position\n",
    "def generate_one_image(image_size):\n",
    "    image = np.zeros((image_size, image_size, 3), dtype=np.float32)\n",
    "    size = np.random.randint(image_size // 4, image_size // 2)  # Random size for digit 1\n",
    "    x_pos = np.random.randint(0, image_size - size)\n",
    "    y_pos = np.random.randint(0, image_size - size)\n",
    "    color = np.random.randint(0, 256, size=3)  # Random color\n",
    "    cv2.line(image, (x_pos + size // 2, y_pos), (x_pos + size // 2, y_pos + size), color.tolist(), thickness=size // 4)\n",
    "    return image\n",
    "\n",
    "# Generate dataset\n",
    "num_samples = 1000  # Number of samples\n",
    "image_size = 28     # Image size\n",
    "\n",
    "X = []  # List to store images\n",
    "y = []  # List to store labels\n",
    "\n",
    "# Generate samples of digit 0\n",
    "for _ in range(num_samples // 2):\n",
    "    image = generate_zero_image(image_size)\n",
    "    X.append(np.transpose(image))\n",
    "    y.append([0])\n",
    "\n",
    "# Generate samples of digit 1\n",
    "for _ in range(num_samples // 2):\n",
    "    image = generate_one_image(image_size)\n",
    "    X.append(np.transpose(image))\n",
    "    y.append([1])\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Shuffle the dataset\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# X is a numpy array of size (Number_of_samplesx28x28x3)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "\n",
    "# plot the first 4 images of the dataset\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 20))\n",
    "for i in range(4):\n",
    "    ax[i].imshow(np.transpose(X[i]))\n",
    "    ax[i].set_title(f\"Label: {y[i][0]}\")\n",
    "    ax[i].axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Pytorch Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to dive into creating a Dataset compatible with PyTorch. This step is crucial as it allows us to integrate our data, represented by X and y, into the PyTorch ecosystem. We will be able to utilize powerful PyTorch tools such as data loaders, which are essential for efficiently handling and processing our dataset during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block of code is used to create a custom dataset class compatible with pytorch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.transform:\n",
    "            X_transform = self.transform(X[idx])\n",
    "            y_transform = self.transform(y[idx])\n",
    "\n",
    "        return X_transform, y_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_tensor doesn't work to convert numpy images into torch tensors\n",
    "\n",
    "transform = torch.from_numpy\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train,\n",
    "                              transform=transform)\n",
    "\n",
    "test_dataset = CustomDataset(X_test, y_test,\n",
    "                             transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction of the CNN solver with a sklearn like API\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def fit(self, train_loader, n_epochs=2):\n",
    "        self.model.train()\n",
    "        for epoch in range(n_epochs):\n",
    "            running_loss = 0.0\n",
    "            compteur = 0\n",
    "            l = len(train_loader)\n",
    "            for X, y in train_loader:\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(X)\n",
    "\n",
    "                # convert the outputs and the labels to float32 to pass it to the loss\n",
    "                outputs = outputs.to(torch.float32)\n",
    "                y = y.to(torch.float32)\n",
    "\n",
    "                loss = self.criterion(outputs, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                if compteur % l  == l-1:\n",
    "                    print(f'[{epoch + 1}, {compteur + 1:5d}]',\n",
    "                          f'train_loss: {running_loss /l :.3f}')\n",
    "                compteur+=1\n",
    "\n",
    "        print('Finished Training')\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for X, _ in test_loader:\n",
    "                inputs = X\n",
    "                outputs = self.model(inputs)\n",
    "                binary_predictions = (outputs >= 0.5).float()\n",
    "                predictions.extend(binary_predictions.cpu().numpy().tolist())\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    # return by default the accuracy score of the model\n",
    "\n",
    "    def score(self, test_loader):\n",
    "        predictions = self.predict(test_loader)\n",
    "        y_true = [y for _, y in test_loader]\n",
    "        y_true = torch.cat(y_true, dim=0).cpu().numpy()\n",
    "        return np.mean(y_true == predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For binary classification, it is a common pratice to use the Binary Cross Entropy loss\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.5)\n",
    "\n",
    "clf = CNN(model=net,\n",
    "          criterion=criterion,\n",
    "          optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] train_loss: 0.196\n",
      "[2,   200] train_loss: 0.174\n",
      "[3,   200] train_loss: 0.155\n",
      "[4,   200] train_loss: 0.138\n",
      "[5,   200] train_loss: 0.124\n",
      "[6,   200] train_loss: 0.110\n",
      "[7,   200] train_loss: 0.099\n",
      "[8,   200] train_loss: 0.089\n",
      "[9,   200] train_loss: 0.081\n",
      "[10,   200] train_loss: 0.073\n",
      "[11,   200] train_loss: 0.066\n",
      "[12,   200] train_loss: 0.060\n",
      "[13,   200] train_loss: 0.055\n",
      "[14,   200] train_loss: 0.051\n",
      "[15,   200] train_loss: 0.047\n",
      "[16,   200] train_loss: 0.043\n",
      "[17,   200] train_loss: 0.040\n",
      "[18,   200] train_loss: 0.037\n",
      "[19,   200] train_loss: 0.035\n",
      "[20,   200] train_loss: 0.032\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "clf.fit(train_loader, n_epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.955"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0006]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = test_dataset[30]\n",
    "\n",
    "# ploting the image\n",
    "\n",
    "plt.imshow(np.transpose(image[0]))\n",
    "\n",
    "# prediction of the model\n",
    "\n",
    "print(clf.model(image[0].unsqueeze(0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLMI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
